# 二者相爱相杀

## Softmax

首先，我们来回顾 **Softmax** 相关内容。大家都知道，对于向量 $(x_1,x_2...,x_n)$，它的 **Softmax** 为：

$$
(p_1,p_2...,p_n)=\frac{1}{\sum_{i=0}^{n}e^{x_i}}(e^{x_1},e^{x_2}...,e^{x_n})
$$
由于 $e^t$ 是关于 $t$ 的严格单调递增函数，所以如果 $x_k$ 是 $(x_1,x_2...,x_n)$ 中的最大者，那么 $p_k$ 也是 $(p_1,p_2...,p_n)$ 中的最大者。

## 交叉熵

对于分类问题，我们所用的 **loss** 一般是交叉熵，也就是：

$$
-log\ p_t =  log(\sum_{i=1}^{n}e^{x_i}) -x_t
$$

其中是 $t$ 是目标类。上式第一项实际上是 $max(x_1,x_2...,x_n)$ 的光滑近似，所以为了形象理解交叉熵，我们可以写出：

$$
-log\ p_t = max(x_1,x_2...,x_n) -x_t
$$

也就是说，交叉熵实际上在缩小目标类得分 $x_t$ 与全局最大值的差距，显然这个差距最小只能为 0，并且此时目标类得分就是最大值者。所以，Softmax 加交叉熵的效果就是“希望目标类的得分成为最大值”。